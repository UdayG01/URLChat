{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPmaL8Rat5Y9WVa7jEoFLCl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UdayG01/URLChat/blob/main/URLChat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDrXoaZlgs6E"
      },
      "outputs": [],
      "source": [
        "# Pinecone key: 58da24ae-2fc8-4c12-97a1-9a4d2ff088df\n",
        "# Replicate key: r8_Ql18CMc7pN46DTrx0Qn8IIHK3RCERrH2ZloZX\n",
        "# Hugging Face key: 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"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain transformers"
      ],
      "metadata": {
        "id": "mJ_fHC-fhobv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install replicate"
      ],
      "metadata": {
        "id": "raa7DO8KiX-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pinecone-client"
      ],
      "metadata": {
        "id": "Y-oZ_4eViZ_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers\n",
        "!pip install xformers\n",
        "!pip install bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "30sKvXIouRQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pinecone\n",
        "import sys\n",
        "from langchain.llms import Replicate\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.document_loaders import WebBaseLoader"
      ],
      "metadata": {
        "id": "dFR8DU5Ahrp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['REPLICATE_API_TOKEN'] = \"r8_Ql18CMc7pN46DTrx0Qn8IIHK3RCERrH2ZloZX\""
      ],
      "metadata": {
        "id": "huDrqTuxis2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pinecone.init(api_key='58da24ae-2fc8-4c12-97a1-9a4d2ff088df', environment='asia-southeast1-gcp-free')"
      ],
      "metadata": {
        "id": "yeQoxH01h6fY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the Doc\n",
        "loader = WebBaseLoader(\"https://medium.com/nlplanet/weekly-ai-and-nlp-news-november-20th-2023-a1533a73ef46\")\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "vCkK6O-9i8hX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "iLJdyiVmkEsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector Embeddings\n",
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "oxMdHTwrkIik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"urlchat\"\n",
        "index = pinecone.Index(index_name)\n",
        "vectordb = Pinecone.from_documents(texts, embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "qfhJJNzRkLYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the LLM\n",
        "llm = Replicate(\n",
        "    model=\"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\",\n",
        "    input={\"temperature\": 0.75, \"max_length\": 3000}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIsgMjSokXk1",
        "outputId": "639ac069-3751-453c-8b71-b1a84d32d116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.replicate:Init param `input` is deprecated, please use `model_kwargs` instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a qa chain using LangChain\n",
        "qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    vectordb.as_retriever(search_kwargs={'k': 2}),\n",
        "    return_source_documents=True\n",
        ")"
      ],
      "metadata": {
        "id": "bwAjDCW8kcCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = []\n",
        "while True:\n",
        "    query = input('Prompt: ')\n",
        "    if query == \"exit\" or query == \"quit\" or query == \"q\":\n",
        "        print('Exiting')\n",
        "        sys.exit()\n",
        "    result = qa_chain({'question': query, 'chat_history': chat_history})\n",
        "    print('Answer: ' + result['answer'] + '\\n')\n",
        "    chat_history.append((query, result['answer']))"
      ],
      "metadata": {
        "id": "_WrrSULPkgzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I believe this video can help me replace cloud llama model taken from Replicate to a free alternative...\n",
        "# \"https://www.youtube.com/watch?v=TcJ_tVSGS4g&ab_channel=MuhammadMoin\""
      ],
      "metadata": {
        "id": "D4BQ7lPNjGGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit App - URLChat"
      ],
      "metadata": {
        "id": "TAEmzni7Tvl-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the dependencies\n",
        "! pip install -q langchain transformers replicate pinecone-client\n",
        "! pip install -q sentence_transformers xformers bitsandbytes accelerate\n",
        "! pip install -q streamlit"
      ],
      "metadata": {
        "id": "r5pzYizXEY2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile htmlTemplates.py\n",
        "\n",
        "\n",
        "css = '''\n",
        "<style>\n",
        ".chat-message {\n",
        "    padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem; display: flex\n",
        "}\n",
        ".chat-message.user {\n",
        "    background-color: #2b313e\n",
        "}\n",
        ".chat-message.bot {\n",
        "    background-color: #475063\n",
        "}\n",
        ".chat-message .avatar {\n",
        "  width: 20%;\n",
        "}\n",
        ".chat-message .avatar img {\n",
        "  max-width: 78px;\n",
        "  max-height: 78px;\n",
        "  border-radius: 50%;\n",
        "  object-fit: cover;\n",
        "}\n",
        ".chat-message .message {\n",
        "  width: 80%;\n",
        "  padding: 0 1.5rem;\n",
        "  color: #fff;\n",
        "}\n",
        "'''\n",
        "\n",
        "bot_template = '''\n",
        "<div class=\"chat-message bot\">\n",
        "    <div class=\"avatar\">\n",
        "        Llama\n",
        "    </div>\n",
        "    <div class=\"message\">{{MSG}}</div>\n",
        "</div>\n",
        "'''\n",
        "\n",
        "user_template = '''\n",
        "<div class=\"chat-message user\">\n",
        "    <div class=\"avatar\">\n",
        "        You\n",
        "    </div>\n",
        "    <div class=\"message\">{{MSG}}</div>\n",
        "</div>\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A-rplTJLL11",
        "outputId": "a17a6b3f-ae4c-44ab-b91b-5557c551676f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing htmlTemplates.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "URL to test the code on:\n",
        "- https://blog.aquasec.com/powerhell-active-flaws-in-powershell-gallery-expose-users-to-attacks\n",
        "- https://medium.com/nlplanet/weekly-ai-and-nlp-news-november-20th-2023-a1533a73ef46"
      ],
      "metadata": {
        "id": "3_HZbw8HMb1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import pinecone\n",
        "import sys\n",
        "from langchain.llms import Replicate\n",
        "from langchain.vectorstores import Pinecone\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "\n",
        "import streamlit as st\n",
        "from htmlTemplates import css, bot_template, user_template\n",
        "\n",
        "os.environ['REPLICATE_API_TOKEN'] = \"r8_Ql18CMc7pN46DTrx0Qn8IIHK3RCERrH2ZloZX\"\n",
        "pinecone.init(api_key='58da24ae-2fc8-4c12-97a1-9a4d2ff088df', environment='asia-southeast1-gcp-free')\n",
        "\n",
        "def load_doc(url):\n",
        "  #loader = WebBaseLoader(\"https://medium.com/@adeigbesharon/i-fell-in-love-with-your-personality-first-828a7bd22d95\")\n",
        "  loader = WebBaseLoader(url)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "def split_text(docs):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "  texts = text_splitter.split_documents(docs)\n",
        "  return texts\n",
        "\n",
        "def get_vectorstore(texts):\n",
        "  embeddings = HuggingFaceEmbeddings()\n",
        "  index_name = \"urlchat\"\n",
        "  index = pinecone.Index(index_name)\n",
        "  db = Pinecone.from_documents(texts, embeddings, index_name=index_name)\n",
        "  return db\n",
        "\n",
        "def get_conversation_chain(db):\n",
        "  llm = Replicate(\n",
        "    model=\"a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5\",\n",
        "    input={\"temperature\": 0.75, \"max_length\": 3000}\n",
        "  )\n",
        "\n",
        "  memory = ConversationBufferMemory(memory_key = 'chat_history', return_messages=True)\n",
        "  conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "      llm=llm,\n",
        "      retriever=db.as_retriever(),\n",
        "      memory=memory\n",
        "  )\n",
        "  return conversation_chain\n",
        "\n",
        "\n",
        "def handle_userinput(user_question):\n",
        "  response = st.session_state.conversation({'question': user_question})\n",
        "  st.session_state.chat_history = response['chat_history']\n",
        "\n",
        "  for i, message in enumerate(st.session_state.chat_history):\n",
        "        if i % 2 == 0:\n",
        "            st.write(user_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "        else:\n",
        "            st.write(bot_template.replace(\n",
        "                \"{{MSG}}\", message.content), unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "  st.set_page_config(page_title=\"ChatLLM.ai\",\n",
        "                    page_icon=\":robot_face:\")\n",
        "  st.write(css, unsafe_allow_html=True)\n",
        "\n",
        "  if \"conversation\" not in st.session_state:\n",
        "    st.session_state.conversation = None\n",
        "  if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = None\n",
        "\n",
        "\n",
        "  st.header(\"URLChat :desktop_computer:\")\n",
        "  user_question = st.text_input(\"What brings you here? \")\n",
        "  if user_question:\n",
        "    handle_userinput(user_question)\n",
        "\n",
        "  with st.sidebar:\n",
        "    st.subheader(\"Your URL\")\n",
        "    form = st.form(\"my_form\")\n",
        "    url = form.text_input(\"Your URL: \")\n",
        "    submit = form.form_submit_button(\"Process\")\n",
        "\n",
        "    if submit:\n",
        "      with st.spinner(\"Process\"):\n",
        "        # Perform loading on the url\n",
        "        docs = load_doc(url)\n",
        "        # st.write(text)\n",
        "\n",
        "        # Perform text-splitting\n",
        "        texts  = split_text(docs)\n",
        "\n",
        "        # Performing embeddings/vectorization using HuggingFaceEmbeddings\n",
        "        # and I'll be storing these embeddings in Chroma vector store\n",
        "        db = get_vectorstore(texts)\n",
        "\n",
        "        # Making a conversation chain\n",
        "        # conversation = get_conversation_chain(db)\n",
        "        # do the below one in case you want a persistent convo\n",
        "        st.session_state.conversation = get_conversation_chain(db)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btFljRYNEWZI",
        "outputId": "cb62118f-938c-4022-adb6-b47e1cd398c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run /content/app.py &>/content/logs.txt &"
      ],
      "metadata": {
        "id": "zCROrBx6K8dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSswikfJK89h",
        "outputId": "fd3306be-ca58-474f-c260-ac0fca5954f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.607s\n",
            "your url is: https://tough-spiders-rhyme.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}